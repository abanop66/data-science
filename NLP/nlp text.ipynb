{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deal with text\n",
    "text='By delivering a unified view of data from numerous sources, data integration simplifies the business intelligence (BI) processes of analysis. Organizations can easily view, and quickly comprehend, the available data sets in order to derive actionable information on the current state of the business. With data integration, analysts can compile more information for more accurate evaluation without being overwhelmed by high volumes. Unlike business analytics, BI doesn’t use predictive analysis to make future projections; instead, it focuses on describing the present and past to aid in strategic decision-making. This use of data integration is well-suited to data warehousing, where high-level overview information in an easily consumable format aligns nicely.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['By delivering a unified view of data from numerous sources, data integration simplifies the business intelligence (BI) processes of analysis.',\n",
       " 'Organizations can easily view, and quickly comprehend, the available data sets in order to derive actionable information on the current state of the business.',\n",
       " 'With data integration, analysts can compile more information for more accurate evaluation without being overwhelmed by high volumes.',\n",
       " 'Unlike business analytics, BI doesn’t use predictive analysis to make future projections; instead, it focuses on describing the present and past to aid in strategic decision-making.',\n",
       " 'This use of data integration is well-suited to data warehousing, where high-level overview information in an easily consumable format aligns nicely.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_sentense=sent_tokenize(text)\n",
    "nltk_sentense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['By',\n",
       " 'delivering',\n",
       " 'a',\n",
       " 'unified',\n",
       " 'view',\n",
       " 'of',\n",
       " 'data',\n",
       " 'from',\n",
       " 'numerous',\n",
       " 'sources',\n",
       " ',',\n",
       " 'data',\n",
       " 'integration',\n",
       " 'simplifies',\n",
       " 'the',\n",
       " 'business',\n",
       " 'intelligence',\n",
       " '(',\n",
       " 'BI',\n",
       " ')',\n",
       " 'processes',\n",
       " 'of',\n",
       " 'analysis',\n",
       " '.',\n",
       " 'Organizations',\n",
       " 'can',\n",
       " 'easily',\n",
       " 'view',\n",
       " ',',\n",
       " 'and',\n",
       " 'quickly',\n",
       " 'comprehend',\n",
       " ',',\n",
       " 'the',\n",
       " 'available',\n",
       " 'data',\n",
       " 'sets',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'derive',\n",
       " 'actionable',\n",
       " 'information',\n",
       " 'on',\n",
       " 'the',\n",
       " 'current',\n",
       " 'state',\n",
       " 'of',\n",
       " 'the',\n",
       " 'business',\n",
       " '.',\n",
       " 'With',\n",
       " 'data',\n",
       " 'integration',\n",
       " ',',\n",
       " 'analysts',\n",
       " 'can',\n",
       " 'compile',\n",
       " 'more',\n",
       " 'information',\n",
       " 'for',\n",
       " 'more',\n",
       " 'accurate',\n",
       " 'evaluation',\n",
       " 'without',\n",
       " 'being',\n",
       " 'overwhelmed',\n",
       " 'by',\n",
       " 'high',\n",
       " 'volumes',\n",
       " '.',\n",
       " 'Unlike',\n",
       " 'business',\n",
       " 'analytics',\n",
       " ',',\n",
       " 'BI',\n",
       " 'doesn',\n",
       " '’',\n",
       " 't',\n",
       " 'use',\n",
       " 'predictive',\n",
       " 'analysis',\n",
       " 'to',\n",
       " 'make',\n",
       " 'future',\n",
       " 'projections',\n",
       " ';',\n",
       " 'instead',\n",
       " ',',\n",
       " 'it',\n",
       " 'focuses',\n",
       " 'on',\n",
       " 'describing',\n",
       " 'the',\n",
       " 'present',\n",
       " 'and',\n",
       " 'past',\n",
       " 'to',\n",
       " 'aid',\n",
       " 'in',\n",
       " 'strategic',\n",
       " 'decision-making',\n",
       " '.',\n",
       " 'This',\n",
       " 'use',\n",
       " 'of',\n",
       " 'data',\n",
       " 'integration',\n",
       " 'is',\n",
       " 'well-suited',\n",
       " 'to',\n",
       " 'data',\n",
       " 'warehousing',\n",
       " ',',\n",
       " 'where',\n",
       " 'high-level',\n",
       " 'overview',\n",
       " 'information',\n",
       " 'in',\n",
       " 'an',\n",
       " 'easily',\n",
       " 'consumable',\n",
       " 'format',\n",
       " 'aligns',\n",
       " 'nicely',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_words=word_tokenize(text)\n",
    "nltk_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# no_punticuation just words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "punticuation_text='i have 4, sons #blessed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'have', '4', 'sons', 'blessed']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "#tokenize_text=RegexpTokenizer(r'\\w+') # \\w+ = [AZaz09]\n",
    "tokenize_text=RegexpTokenizer('[A-Za-z0-9]+')\n",
    "no_punticuation=tokenize_text.tokenize(punticuation_text)\n",
    "no_punticuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =\"an apple a day keeping a doctor away, i will be there for you!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_list=set(stopwords.words('english'))\n",
    "stopwords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an',\n",
       " 'apple',\n",
       " 'a',\n",
       " 'day',\n",
       " 'keeping',\n",
       " 'a',\n",
       " 'doctor',\n",
       " 'away',\n",
       " ',',\n",
       " 'i',\n",
       " 'will',\n",
       " 'be',\n",
       " 'there',\n",
       " 'for',\n",
       " 'you',\n",
       " '!']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_text=word_tokenize(text)\n",
    "tokenize_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text=[]\n",
    "for w in tokenize_text:\n",
    "    if w not in stopwords_list:\n",
    "        new_text.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple', 'day', 'keeping', 'doctor', 'away', ',', '!']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n",
      "inspir\n",
      "طالب\n"
     ]
    }
   ],
   "source": [
    "Porter_Stem=PorterStemmer()\n",
    "print(Porter_Stem.stem('dogs'))\n",
    "print(Porter_Stem.stem('inspiring'))\n",
    "print(Porter_Stem.stem('طالب'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "هو طالب مجتهد جدا\n"
     ]
    }
   ],
   "source": [
    "print(Porter_Stem.stem(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_text=[]\n",
    "for w in word_tokenize(text):\n",
    "    if w not in stopwords_list:\n",
    "        s=Porter_Stem.stem(w)\n",
    "        stem_text.append(s)\n",
    "        stem_text.append(\" \")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "هو طالب مجتهد جدا \n"
     ]
    }
   ],
   "source": [
    "print(\"\".join(stem_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer= WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_text=[]\n",
    "for w in word_tokenize(text):\n",
    "    s=lemmatizer.lemmatize(w,pos=\"v\") # v is verb\n",
    "    stem_text.append(s)\n",
    "    stem_text.append(\" \")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "هو طالب مجتهد جدا \n"
     ]
    }
   ],
   "source": [
    "print(\"\".join(stem_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word embedding .... Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize \n",
    "from nltk.tokenize import RegexpTokenizer #puntcuation\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"https://raw.githubusercontent.com/nitinkaushik01/Natural_Language_Processing/master/IMDB_Dataset.csv\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data_list=list()\n",
    "\n",
    "indv_lines=data[\"review\"].values.tolist()\n",
    "\n",
    "for line in indv_lines:\n",
    "    #remove punticuation from token\n",
    "    rem_pun_token=RegexpTokenizer('[A-Za-z0-9]+')\n",
    "    token=rem_pun_token.tokenize(line)\n",
    "    #convert words to lower case\n",
    "    words=[w.lower() for w in token]\n",
    "    #stopwords\n",
    "    stopwords_list=set(stopwords.words('english'))\n",
    "    words=[w for w in words if not w in stopwords_list]\n",
    "    #add words to review list\n",
    "    review_data_list.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['one',\n",
       "  'reviewers',\n",
       "  'mentioned',\n",
       "  'watching',\n",
       "  '1',\n",
       "  'oz',\n",
       "  'episode',\n",
       "  'hooked',\n",
       "  'right',\n",
       "  'exactly',\n",
       "  'happened',\n",
       "  'br',\n",
       "  'br',\n",
       "  'first',\n",
       "  'thing',\n",
       "  'struck',\n",
       "  'oz',\n",
       "  'brutality',\n",
       "  'unflinching',\n",
       "  'scenes',\n",
       "  'violence',\n",
       "  'set',\n",
       "  'right',\n",
       "  'word',\n",
       "  'go',\n",
       "  'trust',\n",
       "  'show',\n",
       "  'faint',\n",
       "  'hearted',\n",
       "  'timid',\n",
       "  'show',\n",
       "  'pulls',\n",
       "  'punches',\n",
       "  'regards',\n",
       "  'drugs',\n",
       "  'sex',\n",
       "  'violence',\n",
       "  'hardcore',\n",
       "  'classic',\n",
       "  'use',\n",
       "  'word',\n",
       "  'br',\n",
       "  'br',\n",
       "  'called',\n",
       "  'oz',\n",
       "  'nickname',\n",
       "  'given',\n",
       "  'oswald',\n",
       "  'maximum',\n",
       "  'security',\n",
       "  'state',\n",
       "  'penitentary',\n",
       "  'focuses',\n",
       "  'mainly',\n",
       "  'emerald',\n",
       "  'city',\n",
       "  'experimental',\n",
       "  'section',\n",
       "  'prison',\n",
       "  'cells',\n",
       "  'glass',\n",
       "  'fronts',\n",
       "  'face',\n",
       "  'inwards',\n",
       "  'privacy',\n",
       "  'high',\n",
       "  'agenda',\n",
       "  'em',\n",
       "  'city',\n",
       "  'home',\n",
       "  'many',\n",
       "  'aryans',\n",
       "  'muslims',\n",
       "  'gangstas',\n",
       "  'latinos',\n",
       "  'christians',\n",
       "  'italians',\n",
       "  'irish',\n",
       "  'scuffles',\n",
       "  'death',\n",
       "  'stares',\n",
       "  'dodgy',\n",
       "  'dealings',\n",
       "  'shady',\n",
       "  'agreements',\n",
       "  'never',\n",
       "  'far',\n",
       "  'away',\n",
       "  'br',\n",
       "  'br',\n",
       "  'would',\n",
       "  'say',\n",
       "  'main',\n",
       "  'appeal',\n",
       "  'show',\n",
       "  'due',\n",
       "  'fact',\n",
       "  'goes',\n",
       "  'shows',\n",
       "  'dare',\n",
       "  'forget',\n",
       "  'pretty',\n",
       "  'pictures',\n",
       "  'painted',\n",
       "  'mainstream',\n",
       "  'audiences',\n",
       "  'forget',\n",
       "  'charm',\n",
       "  'forget',\n",
       "  'romance',\n",
       "  'oz',\n",
       "  'mess',\n",
       "  'around',\n",
       "  'first',\n",
       "  'episode',\n",
       "  'ever',\n",
       "  'saw',\n",
       "  'struck',\n",
       "  'nasty',\n",
       "  'surreal',\n",
       "  'say',\n",
       "  'ready',\n",
       "  'watched',\n",
       "  'developed',\n",
       "  'taste',\n",
       "  'oz',\n",
       "  'got',\n",
       "  'accustomed',\n",
       "  'high',\n",
       "  'levels',\n",
       "  'graphic',\n",
       "  'violence',\n",
       "  'violence',\n",
       "  'injustice',\n",
       "  'crooked',\n",
       "  'guards',\n",
       "  'sold',\n",
       "  'nickel',\n",
       "  'inmates',\n",
       "  'kill',\n",
       "  'order',\n",
       "  'get',\n",
       "  'away',\n",
       "  'well',\n",
       "  'mannered',\n",
       "  'middle',\n",
       "  'class',\n",
       "  'inmates',\n",
       "  'turned',\n",
       "  'prison',\n",
       "  'bitches',\n",
       "  'due',\n",
       "  'lack',\n",
       "  'street',\n",
       "  'skills',\n",
       "  'prison',\n",
       "  'experience',\n",
       "  'watching',\n",
       "  'oz',\n",
       "  'may',\n",
       "  'become',\n",
       "  'comfortable',\n",
       "  'uncomfortable',\n",
       "  'viewing',\n",
       "  'thats',\n",
       "  'get',\n",
       "  'touch',\n",
       "  'darker',\n",
       "  'side'],\n",
       " ['wonderful',\n",
       "  'little',\n",
       "  'production',\n",
       "  'br',\n",
       "  'br',\n",
       "  'filming',\n",
       "  'technique',\n",
       "  'unassuming',\n",
       "  'old',\n",
       "  'time',\n",
       "  'bbc',\n",
       "  'fashion',\n",
       "  'gives',\n",
       "  'comforting',\n",
       "  'sometimes',\n",
       "  'discomforting',\n",
       "  'sense',\n",
       "  'realism',\n",
       "  'entire',\n",
       "  'piece',\n",
       "  'br',\n",
       "  'br',\n",
       "  'actors',\n",
       "  'extremely',\n",
       "  'well',\n",
       "  'chosen',\n",
       "  'michael',\n",
       "  'sheen',\n",
       "  'got',\n",
       "  'polari',\n",
       "  'voices',\n",
       "  'pat',\n",
       "  'truly',\n",
       "  'see',\n",
       "  'seamless',\n",
       "  'editing',\n",
       "  'guided',\n",
       "  'references',\n",
       "  'williams',\n",
       "  'diary',\n",
       "  'entries',\n",
       "  'well',\n",
       "  'worth',\n",
       "  'watching',\n",
       "  'terrificly',\n",
       "  'written',\n",
       "  'performed',\n",
       "  'piece',\n",
       "  'masterful',\n",
       "  'production',\n",
       "  'one',\n",
       "  'great',\n",
       "  'master',\n",
       "  'comedy',\n",
       "  'life',\n",
       "  'br',\n",
       "  'br',\n",
       "  'realism',\n",
       "  'really',\n",
       "  'comes',\n",
       "  'home',\n",
       "  'little',\n",
       "  'things',\n",
       "  'fantasy',\n",
       "  'guard',\n",
       "  'rather',\n",
       "  'use',\n",
       "  'traditional',\n",
       "  'dream',\n",
       "  'techniques',\n",
       "  'remains',\n",
       "  'solid',\n",
       "  'disappears',\n",
       "  'plays',\n",
       "  'knowledge',\n",
       "  'senses',\n",
       "  'particularly',\n",
       "  'scenes',\n",
       "  'concerning',\n",
       "  'orton',\n",
       "  'halliwell',\n",
       "  'sets',\n",
       "  'particularly',\n",
       "  'flat',\n",
       "  'halliwell',\n",
       "  'murals',\n",
       "  'decorating',\n",
       "  'every',\n",
       "  'surface',\n",
       "  'terribly',\n",
       "  'well',\n",
       "  'done'],\n",
       " ['thought',\n",
       "  'wonderful',\n",
       "  'way',\n",
       "  'spend',\n",
       "  'time',\n",
       "  'hot',\n",
       "  'summer',\n",
       "  'weekend',\n",
       "  'sitting',\n",
       "  'air',\n",
       "  'conditioned',\n",
       "  'theater',\n",
       "  'watching',\n",
       "  'light',\n",
       "  'hearted',\n",
       "  'comedy',\n",
       "  'plot',\n",
       "  'simplistic',\n",
       "  'dialogue',\n",
       "  'witty',\n",
       "  'characters',\n",
       "  'likable',\n",
       "  'even',\n",
       "  'well',\n",
       "  'bread',\n",
       "  'suspected',\n",
       "  'serial',\n",
       "  'killer',\n",
       "  'may',\n",
       "  'disappointed',\n",
       "  'realize',\n",
       "  'match',\n",
       "  'point',\n",
       "  '2',\n",
       "  'risk',\n",
       "  'addiction',\n",
       "  'thought',\n",
       "  'proof',\n",
       "  'woody',\n",
       "  'allen',\n",
       "  'still',\n",
       "  'fully',\n",
       "  'control',\n",
       "  'style',\n",
       "  'many',\n",
       "  'us',\n",
       "  'grown',\n",
       "  'love',\n",
       "  'br',\n",
       "  'br',\n",
       "  'laughed',\n",
       "  'one',\n",
       "  'woody',\n",
       "  'comedies',\n",
       "  'years',\n",
       "  'dare',\n",
       "  'say',\n",
       "  'decade',\n",
       "  'never',\n",
       "  'impressed',\n",
       "  'scarlet',\n",
       "  'johanson',\n",
       "  'managed',\n",
       "  'tone',\n",
       "  'sexy',\n",
       "  'image',\n",
       "  'jumped',\n",
       "  'right',\n",
       "  'average',\n",
       "  'spirited',\n",
       "  'young',\n",
       "  'woman',\n",
       "  'br',\n",
       "  'br',\n",
       "  'may',\n",
       "  'crown',\n",
       "  'jewel',\n",
       "  'career',\n",
       "  'wittier',\n",
       "  'devil',\n",
       "  'wears',\n",
       "  'prada',\n",
       "  'interesting',\n",
       "  'superman',\n",
       "  'great',\n",
       "  'comedy',\n",
       "  'go',\n",
       "  'see',\n",
       "  'friends'],\n",
       " ['basically',\n",
       "  'family',\n",
       "  'little',\n",
       "  'boy',\n",
       "  'jake',\n",
       "  'thinks',\n",
       "  'zombie',\n",
       "  'closet',\n",
       "  'parents',\n",
       "  'fighting',\n",
       "  'time',\n",
       "  'br',\n",
       "  'br',\n",
       "  'movie',\n",
       "  'slower',\n",
       "  'soap',\n",
       "  'opera',\n",
       "  'suddenly',\n",
       "  'jake',\n",
       "  'decides',\n",
       "  'become',\n",
       "  'rambo',\n",
       "  'kill',\n",
       "  'zombie',\n",
       "  'br',\n",
       "  'br',\n",
       "  'ok',\n",
       "  'first',\n",
       "  'going',\n",
       "  'make',\n",
       "  'film',\n",
       "  'must',\n",
       "  'decide',\n",
       "  'thriller',\n",
       "  'drama',\n",
       "  'drama',\n",
       "  'movie',\n",
       "  'watchable',\n",
       "  'parents',\n",
       "  'divorcing',\n",
       "  'arguing',\n",
       "  'like',\n",
       "  'real',\n",
       "  'life',\n",
       "  'jake',\n",
       "  'closet',\n",
       "  'totally',\n",
       "  'ruins',\n",
       "  'film',\n",
       "  'expected',\n",
       "  'see',\n",
       "  'boogeyman',\n",
       "  'similar',\n",
       "  'movie',\n",
       "  'instead',\n",
       "  'watched',\n",
       "  'drama',\n",
       "  'meaningless',\n",
       "  'thriller',\n",
       "  'spots',\n",
       "  'br',\n",
       "  'br',\n",
       "  '3',\n",
       "  '10',\n",
       "  'well',\n",
       "  'playing',\n",
       "  'parents',\n",
       "  'descent',\n",
       "  'dialogs',\n",
       "  'shots',\n",
       "  'jake',\n",
       "  'ignore']]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data_list[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "embedding_dim=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train word2vector model\n",
    "model =gensim.models.Word2Vec(sentences=review_data_list, vector_size= embedding_dim, workers=4,min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=list(model.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"amazing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"awful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#like queen + man - women = king\n",
    "model.wv.most_similar_cosmul(positive=[\"king\",\"woman\"],negative=[\"man\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find odd word\n",
    "print(model.wv.doesnt_match(\"king queen cat soldier\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syns=wordnet.synsets(\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syns[0].lemmas()[0].name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms=[]\n",
    "antonyms=[]\n",
    " \n",
    "for syn in wordnet.synsets(\"good\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1=wordnet.synset('ship.n.01')\n",
    "w2=wordnet.synset('boat.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1.wup_similarity(w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown,movie_reviews,reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews.categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adj or noun or verb ....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"هو طالب مجتهد جد\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('هو', 'JJ'), ('طالب', 'NNP'), ('مجتهد', 'NNP'), ('جد', 'NN')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_text=[]\n",
    "for w in word_tokenize(text):\n",
    "    s=lemmatizer.lemmatize(w,pos=\"v\") # v is verb\n",
    "    stem_text.append(s)\n",
    "    stem_text.append(\" \")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['هو', ' ', 'طالب', ' ', 'مجتهد', ' ', 'جد', ' ']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Standard IR Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'y', 'x', 'b', 'c', 'd', 'e', 'f']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference=\"a y x b c d e f\".split()\n",
    "reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd', 'g', 'h', 'e', 'f']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=\"a b c d g h e f\".split()\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.375"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(reference,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_distance(\"close\",\"clothing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
